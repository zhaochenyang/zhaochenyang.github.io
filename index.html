<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="remember &amp; think">
<meta property="og:type" content="website">
<meta property="og:title" content="记忆小溪">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="记忆小溪">
<meta property="og:description" content="remember &amp; think">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="记忆小溪">
<meta name="twitter:description" content="remember &amp; think">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>记忆小溪</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">记忆小溪</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">总结与感悟</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/26/Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="zhaochenyang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="记忆小溪">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/26/Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations/" itemprop="url">Untitled</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-26T18:47:01+08:00">
                2019-10-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Sampling-Bias-Corrected-Neural-Modeling-for-Large-Corpus-Item-Recommendations"><a href="#Sampling-Bias-Corrected-Neural-Modeling-for-Large-Corpus-Item-Recommendations" class="headerlink" title="Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations"></a>Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations</h3><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>许多推荐系统在一个非常大的库中召回(retrieve)和评分(score)物料(item)。通常处理数据稀疏和指数item分布的方法是从它的内容特征中学习item 表示。与基于矩阵分解的许多content-aware 系统不同，我们考虑了用双塔神经网络模型框架，其中一个塔（item 塔）编码不同的item内容特征。一个通用的方法是训练这样一个双塔来优化loss函数，批量负样本，这些item从随机最小batch中采样而来。然而in-batch loss受限于采样bias，潜在影响模型表现，特别是在扭曲严重的分布中。本文我们给出了一个原创算法预估流式数据中的item频率。通过理论分析和模拟，我们展示了提出的算法无需固定的item vocabulary而奏效，并且有能力产生无偏的估计且能够适应item分布变化。我们然后应用采样偏置更正（sampling-bias-corrected ）建模方法来建立一个大规模神经召回系统，来给YouTube推荐使用。系统部署用于从一个千万级video库中召回个性化建议。我们通过两个真实存在的数据集的离线实验证明采样偏置更正的有效性。我们也进行了线上AB测试，来展示神经召回系统导致的YouTube推荐质量提升。</p>
<h4 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h4><p>推荐系统帮助用户发现感兴趣的内容，通过许多internet服务，包含视频推荐，aap建议，和在线广告命中。在多种情况下，这些系统关联十亿级用户和大规模内容的item通常是百万到十亿，在严格的时延要求下。一个通常的是现实将推荐当做是retieval和ranking问题。然后设计一个两阶段系统。这是一个可扩展的Retrieval模型先从一个大的库中retrieve一个小规模的相关item，然后一个充分发挥的ranking模型对retrieved到的item进行rerank，根据一个或多个目标例如点击或者用户评分。在这些工作中，我们关注于建立一个实际世界中学到的召回系统能够扩展到百万级别item的个性化推荐系统。</p>
<p>给定一个三元组{user， context，item}，一个通用的解法是建立一个可扩展的retieval模型</p>
<ol>
<li><p>学习查询和item的表征，分别对应{user,context}和{item}</p>
</li>
<li><p>用一个简单评分函数，如点击，在query和item表示，来获得query的合适的推荐。</p>
</li>
</ol>
<p>context通常表示动态变量，例如时间天，user使用的device。表示学习问题有两个典型挑战：</p>
<ol>
<li>item的库可能极大，对于许多工业级的应用来讲</li>
<li>从用户反馈收集到的用户数据对于大多数item通常非常稀疏，因此导致模型预测对于长尾内容有个非常大的variance。</li>
</ol>
<p>面对充分报道的冷启动问题，真实世界系统需要使用数据分布变化来是新鲜内容更好的曝光出来。</p>
<p>受到Netflix prize启发，基于矩阵分解建模已经被广泛接纳，学习query和item的隐向量，用户建立retieval 系统。在MF框架下，推荐研究的主题处理上述挑战从一个大的库中。这些通用思想是利用query和item的content features。content features能够被简单定义为各种描述item的features，不仅限于item id。例如一个video的content feature能是visual和audio feature，从视频关键帧中抽出来的。基于MF的模型经常能够捕捉二阶交叉特征，因此这有有限的能量表达一个各种形式的特征集合。</p>
<p>近年来，在CV和NLP领域深度学习的成功激发，有大量的工作是应用DNN做推荐。深度表示很适合编码复杂的用书状态和item内容特征，表示成低维度嵌入空间。本论文中，我们探索了双塔DNN在建立retieval模型中。</p>
<p>结果，双塔模型架构能够建模标签有结构或者内容features。MLP模型通常用许多负采样样本，从一个固定item vocabulary中。相反，用deep item tower，典型的无效的，采样和训练在许多负样本，由于item content feature和共享的网络参数来计算所有item embedding。</p>
<p>我们考虑batch softmax optimization，item的概率是由所有的随机batch中item的全量计算的，作为一个训练双塔DNN的通用方法。然而正如我们实验展示的，batch softmax受限于采样偏置并且能够显著限制模型效果，如果没有修正。重要性采样和相关偏置减少在MLP模型中被研究过了。由这些工作启发，我们提出了更正采样偏置，在batch softmax中使用估计item频率。与MLP模型不同，那些模型的输出item vocabulary是固定的，我们目标是流式数据情况，随着时间vocabulary和分布变化。我们提出了一个原创的算法，统计与估计item 频率通过gradient descent。另外，我们应用偏置更正建模，建立一个YouTube个性化推荐系统。我们也介绍了序列训练策略，设计包含流式数据，和系统serving与索引。</p>
<p>主要贡献：</p>
<ul>
<li>流式频率预估</li>
<li>建模框架</li>
<li>YouTube推荐</li>
<li>离线和线上实验</li>
</ul>
<h4 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2 相关工作"></a>2 相关工作</h4><p>本章，我们给了相关工作的的一个概览，重点是和我们贡献的联系。</p>
<h5 id="2-1-Content-Aware-and-Neural-Recommenders"><a href="#2-1-Content-Aware-and-Neural-Recommenders" class="headerlink" title="2.1 Content-Aware and Neural Recommenders"></a>2.1 Content-Aware and Neural Recommenders</h5><p>利用user和item的content features对于提高泛化性和减轻冷启动问题十分关键。有一些列研究聚焦在将content features加入到经典的MF框架中。例如泛化MF模型，SVDFeature FM能够应用，加入item content features。这些模型能够抓住双线性，如特征间二阶交叉特征。最近这些年DNN被证明在提升推荐准确率方面更有效。由于高度非线性的有事，DNN提供了一个较大的容量能够抓住复杂特征交叉，比传统的范姐方法。</p>
<p>与NCF不同两方面：</p>
<ol>
<li>我们提升了双塔网络来建模user-item交互，因此能够在一个近似线性时间完成一个大库item的推断。学习NCF使用point-wise loss，例如squared or log loss。这里我们介绍了一个multi-class soft-max loss并且显示模拟item 频率。</li>
</ol>
<h5 id="2-2-极限分类"><a href="#2-2-极限分类" class="headerlink" title="2.2 极限分类"></a>2.2 极限分类</h5><p>softmax是一个最常用的函数，在设计一个百万标签大的输出空间的预测时。许多研究已经聚焦在训练softmax分类模型，对于一个大量的类别，从语言任务到推荐。当类别极限大时，一个广泛使用的技术来加速悬链的方法是在一个类别的子集中进行采样。Bengio展示了一个好的采样分布应该是和模型的输出分布相适应的。为了避免计算采样分布的的复杂化，许多实际的模型使用了一个简单的分布例如unigram或均匀采样作为替代。最近Blanc等设计了一个有效和适应性的基于kernel的采样方法。不管采样softmax的各种变种的成功，这里对于label有content feautres的并不适用。适应性的采样任然是一个待解决问题。各种工作证明基于树的边界结构，例如级联softmax是很有用的，对于建立大规模分类模型，在降低推断时间方面是很显著的。这些方法通常需要一个与定义的树结构，基于某些特定的类别属性，结果他们并不合适，对于合并很多种输入features。</p>
<h5 id="2-3-Two-tower-Models"><a href="#2-3-Two-tower-Models" class="headerlink" title="2.3 Two-tower Models"></a>2.3 Two-tower Models</h5><p>建立两个塔的神经网络在不同的语言任务中最近已经成为流行的方法了，包含语义相似建模，问答建议和基于文本的信息召回。我们的工作贡献，对于这一些列的研究，特别地证明了双塔模型在建立了大规模推荐系统的有效性。对于许多上文提到的许多语言任务，他是值得注意的，我们聚焦了一个大规模的库，对于YouTube这样的应用是常见的，通过线上实验，我们找到了显示的模拟item频率的关键是提升retieval准确率在设置中，在现在的公国中还么有被有效解决。</p>
<h4 id="3-模型框架"><a href="#3-模型框架" class="headerlink" title="3 模型框架"></a>3 模型框架</h4><p>我们考虑了一个推荐问题的设定，我们有一些列的query和item的集合，queries和item是用x和y分别表示，这里 x y都是一个很多feaure的合集，例如sparse ID和dense feature，并且可能在一个非常高维的空间。目标是，在一个item子集中retrieval item。在个性化场景，我们假定user和context是在x中完全捕捉到信息中。我们开始用一个有限的query和item来解释直觉。我们的模型框剪没有在那样的交涉下工作。</p>
<p>我们目的是建立一个模型，使用两个参数化的embedding函数能够映射模型参数和特征</p>
<p>T := {(xi, i,ri)}Ti=1,</p>
<p>ri能够扩展成捕捉各种维度的用户和一个候选集的交互行为。例如，新闻土建中，ri能够使用户在某个特定文章上所花的时间。给定一个query x，一个对于M个item选择y个item的概率分布的通常的选择事基于softmax函数，如下</p>
<p><img src="/Users/chenyangzhao/Library/Application Support/typora-user-images/image-20191103102421196.png" alt="image-20191103102421196"></p>
<p>对于更深入的考虑ri，我们考虑如下的加权log-likelihood作为loss 函数</p>
<p><img src="/Users/chenyangzhao/Library/Application Support/typora-user-images/image-20191103102551198.png" alt="image-20191103102551198"></p>
<p>当M非常大的时候，对于包含所有的候选样本计算partition函数是不太可行的，例如，这里的分母部分。一个通常的想法是，使用一个item的自己建立分部函数。我们关注处理流式数据。结果，不想训练MLP模型里负反馈是从一个固定的语料库中选择一样，我们考虑使用batch内的item作为所有batch内的query的负反馈。精确的将，给定一个mini-batch B，{(xi, i,ri)}B i=1,对于B中多有的i，这里的batch softmax是<img src="/Users/chenyangzhao/Library/Application Support/typora-user-images/image-20191103103026925.png" alt="image-20191103103026925"></p>
<p>batch内的item通常从一个我们目标应用的幂次分布中采样而来。结果公式3中引入了一个非常大的bias，相对于full softmax，流行的item由于他在batch内出现的几率教改而被过度惩罚了。受到logQ修正的启发，采样sample softmax模型，我们改正了每个logits s用一下的等式</p>
<p><img src="/Users/chenyangzhao/Library/Application Support/typora-user-images/image-20191103103405676.png" alt="image-20191103103405676"></p>
<p>这里pj代表item j的采样概率在一个random batch中。我们保留pj的估计到下一个部分。</p>
<p>加上修真过之后我们有</p>
<p><img src="/Users/chenyangzhao/Library/Application Support/typora-user-images/image-20191103104303010.png" alt="image-20191103104303010"></p>
<p>然后将上述概率带入公式二。</p>
<p>这里是batch loss function。使用SGD with lr r进行模型参数更新。</p>
<p><img src="/Users/chenyangzhao/Library/Application Support/typora-user-images/image-20191103104424614.png" alt="image-20191103104424614"></p>
<p>这里LBuxuyao一个固定的query或候选集。相关的，公式5能够被应用到流式训练数据，他们的数据是随着时间变化的。看算法1是我们提出算法的全部描述。</p>
<p>NNS:一旦embedding 函数u v学号后，推断包含两步</p>
<ol>
<li><p>计算query embedding u(x,theta)</p>
</li>
<li><p>进行近邻查找，在一个item embedding集合上，这些embedding从函数v中预训练出来。</p>
<p>更多，我们的建模框架提供了选项选择任意数据集在inference的时候。</p>
<p>使用高效相似度查找系统，建立在hash技术基础上，不会与全部item计算点积给出top item。对于近似最大内积查找系统问题。具体的，紧凑的高维embedding表示通过量化和端到端学习的粗糙的和点积量化。</p>
</li>
</ol>
<p>Normalization and Temperature. 经验性的，我们发现增加embedding 归一化，除以二范式u(x, ) u(x, )/ku(x, )k2, ( , )  ( , )/k ( , )k2 ，提高了模型可训练性，并且导致了更好的查找质量。另外，temperature t加到每个logit来sharpen predictions，即</p>
<p><img src="/Users/chenyangzhao/Library/Application Support/typora-user-images/image-20191103184119289.png" alt="image-20191103184119289"></p>
<p>实际中t是一个超参数，调整到最大化例如recall 或precision之类的retrieval metics</p>
<p>g</p>
<h4 id="4-流式频率预估"><a href="#4-流式频率预估" class="headerlink" title="4 流式频率预估"></a>4 流式频率预估</h4><p>本部分，我们详述流式频率预估。假设有一个随机batch流，这里每个batch包含一个item集合。这个问题是估计每个itemy在每个batch中的点击概率。一个关键的设计标准是有一个高度分布式估计来支撑分布式训练，当有多个训练任务，workers。在每台机器上或者分布式徐脸上，有个唯一全局step，这表示全局唯一step，这个global step代表训练器消费的数据batch的数量。在分布式设置中，global step通常在多个worker之间同步在通过多个ps方式。我们利用global step并且转化一个item的频次估计到 theta，这表示item在连个连续两次点击的平均step 数量。例如一个item每50 步采样一次，然后我们得到p=0.02。global step的使用给我们两个优势：</p>
<ol>
<li>多个worker饮食的同步频率估计，通过读取和次改全局step</li>
<li>估计theta能够实现，一个简单的平均更新，适应分布变化</li>
</ol>
<p>使用固定item vocabulary 不能现实，我们使用hash array来几率流式id的采样信息。引入的hash 坑呢会引起潜在的hash 碰撞。我们重新看下issue，并提出了一个改进的算法。正如算法2所示，我们用两个arrayA和B，大小为H。鸡舍h是hash函数，能够将任何item映射到一个H空间的interger，这个映射可以基于ID或者其他简(要特征值。然后给一个item y，A[h(y)]记录最新的部署，当y被采样，B[h(y)]包含了对y的theta估计，饿哦们使用A更新B。一旦itemy在第t步更新了，我们更新arrayB：</p>
<p><img src="/Users/chenyangzhao/Library/Application Support/typora-user-images/image-20191103193512470.png" alt="image-20191103193512470"></p>
<p>当B更新了，我们将t赋给A[h(y)].</p>
<p>对于每个item，假设step值在两个连续点击用随机变量delta表示，均值E(delta)。这里我们的目标是估计theta从一个流式采样中。当一个item出现在某个stept中，t   A[ v(y )]是一个新的delta样本。相关的上述更新能够使用SGD使用固定learning  rate alpha 来学习这个随机值的均值。公式化的，在无冲突情况下，下一个结果显示了bias和variance，这个线上估计。</p>
<p>假设 delte1 2 … t是一些列独立荣分布的随机变量采样。让delta等于其均值，假设线上估计这里i属于t 并且alpha属于（0,1）</p>
<p><img src="/Users/chenyangzhao/Library/Application Support/typora-user-images/image-20191103200255037.png" alt="image-20191103200255037"></p>
<p>z</p>
<p>z</p>
<p>k</p>
<h4 id="5-YouTube神经召回系统"><a href="#5-YouTube神经召回系统" class="headerlink" title="5 YouTube神经召回系统"></a>5 YouTube神经召回系统</h4><h5 id="5-1-模型概览"><a href="#5-1-模型概览" class="headerlink" title="5.1 模型概览"></a>5.1 模型概览</h5><h5 id="5-2-序列训练"><a href="#5-2-序列训练" class="headerlink" title="5.2 序列训练"></a>5.2 序列训练</h5><h5 id="5-3-索引和模型serving"><a href="#5-3-索引和模型serving" class="headerlink" title="5.3 索引和模型serving"></a>5.3 索引和模型serving</h5><h4 id="6-实验"><a href="#6-实验" class="headerlink" title="6 实验"></a>6 实验</h4><h5 id="6-1-频率预估模拟"><a href="#6-1-频率预估模拟" class="headerlink" title="6.1 频率预估模拟"></a>6.1 频率预估模拟</h5><h5 id="6-2-Wikipedia-page-Retrieval"><a href="#6-2-Wikipedia-page-Retrieval" class="headerlink" title="6.2 Wikipedia page Retrieval"></a>6.2 Wikipedia page Retrieval</h5><h5 id="6-3-YouTube实验"><a href="#6-3-YouTube实验" class="headerlink" title="6.3 YouTube实验"></a>6.3 YouTube实验</h5><h4 id="7-总结"><a href="#7-总结" class="headerlink" title="7 总结"></a>7 总结</h4><h4 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h4><p>REFERENCES </p>
<ol>
<li>[1]  Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jefrey Dean, Matthieu Devin, San- jay Ghemawat, Ian Goodfellow, Andrew Harp, Geofrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. <a href="https://www.tensorfow.org/" target="_blank" rel="noopener">https://www.tensorfow.org/</a> Software available from tensorfow.org. </li>
<li>[2]  Alexandr Andoni and Piotr Indyk. 2008. Near-optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions. Commun. ACM 51, 1 (Jan. 2008), 117–122. <a href="https://doi.org/10.1145/1327452.1327494" target="_blank" rel="noopener">https://doi.org/10.1145/1327452.1327494</a> </li>
<li>[3]  Immanuel Bayer, Xiangnan He, Bhargav Kanagal, and Stefen Rendle. 2017. A Generic Coordinate Descent Framework for Learning from Implicit Feedback. In Proceedings of the 26th International Conference on World Wide Web (WWW ’17). 1341–1350. </li>
<li>[4]  Yoshua Bengio and Jean-Sébastien Sénécal. 2003. Quick Training of Probabilistic Neural Nets by Importance Sampling. In Proceedings of the conference on Artifcial Intelligence and Statistics (AISTATS). </li>
<li>[5]  Y. Bengio and J. S. Senecal. 2008. Adaptive Importance Sampling to Accelerate Training of a Neural Probabilistic Language Model. Trans. Neur. Netw. 19, 4 (April 2008), 713–722. <a href="https://doi.org/10.1109/TNN.2007.912312" target="_blank" rel="noopener">https://doi.org/10.1109/TNN.2007.912312</a> </li>
<li>[6]  AlexBeutel,PaulCovington,SagarJain,CanXu,JiaLi,VinceGatto,andEdH. Chi. 2018. Latent Cross: Making Use of Context in Recurrent Recommender Systems. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining (WSDM ’18). ACM, New York, NY, USA, 46–54. https: //doi.org/10.1145/3159652.3159727 </li>
<li>[7]  Guy Blanc and Stefen Rendle. 2018. Adaptive Sampled Softmax with Kernel Based Sampling. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018. 589–598. <a href="http://proceedings.mlr.press/v80/blanc18a.html" target="_blank" rel="noopener">http://proceedings.mlr.press/v80/blanc18a.html</a> </li>
<li>[8]  TianqiChen,WeinanZhang,QiuxiaLu,KailongChen,ZhaoZheng,andYong Yu. 2012. SVDFeature: A Toolkit for Feature-based Collaborative Filtering. J. Mach. Learn. Res. 13, 1 (Dec. 2012), 3619–3622. <a href="http://dl.acm.org/citation.cfm?" target="_blank" rel="noopener">http://dl.acm.org/citation.cfm?</a> id=2503308.2503357 </li>
<li>[9]  Heng-TzeCheng,LeventKoc,JeremiahHarmsen,TalShaked,TusharChandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah. 2016. Wide Deep Learning for Recommender Systems. arXiv:1606.07792 (2016). <a href="http://arxiv.org/abs/1606.07792" target="_blank" rel="noopener">http://arxiv.org/abs/1606.07792</a> </li>
<li>[10]  Edith Cohen and David D. Lewis. 1997. Approximating Matrix Multiplication for Pattern Recognition Tasks. In Proceedings of the Eighth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA ’97). Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, 682–691. <a href="http://dl.acm.org/citation.cfm?" target="_blank" rel="noopener">http://dl.acm.org/citation.cfm?</a> id=314161.314415 </li>
<li>[11]  Graham Cormode and S. Muthukrishnan. 2005. An Improved Data Stream Summary: The Count-min Sketch and Its Applications. J. Algorithms 55, 1 (April 2005), 58–75. <a href="https://doi.org/10.1016/j.jalgor.2003.12.001" target="_blank" rel="noopener">https://doi.org/10.1016/j.jalgor.2003.12.001</a> </li>
<li>[12]  Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep Neural Networks for YouTube Recommendations. In Proceedings of the 10th ACM Conference on Recommender Systems. New York, NY, USA. </li>
<li>[13]  JefreyDean,GregS.Corrado,RajatMonga,KaiChen,MatthieuDevin,QuocV. Le, Mark Z. Mao, Marc’Aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, and Andrew Y. Ng. 2012. Large Scale Distributed Deep Networks. In NIPS. </li>
<li>[14]  Tim Donkers, Benedikt Loepp, and Jürgen Ziegler. 2017. Sequential User-based Recurrent Neural Network Recommendations. In Proceedings of the Eleventh ACM Conference on Recommender Systems (RecSys ’17). ACM, New York, NY, USA, 152–160. <a href="https://doi.org/10.1145/3109859.3109877" target="_blank" rel="noopener">https://doi.org/10.1145/3109859.3109877</a> </li>
<li>[15]  John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. J. Mach. Learn. Res. 12 (July 2011), 2121–2159. <a href="http://dl.acm.org/citation.cfm?id=1953048.2021068" target="_blank" rel="noopener">http://dl.acm.org/citation.cfm?id=1953048.2021068</a> </li>
<li>[16]  Wikimedia Foundation. [n.d.]. Wikimedia Downloads. <a href="https://dumps.wikimedia" target="_blank" rel="noopener">https://dumps.wikimedia</a>. org/ </li>
<li>[17]  Daniel Gillick, Alessandro Presta, and Gaurav Singh Tomar. 2018. End-to-End Retrieval in Continuous Space. CoRR abs/1811.08008 (2018). arXiv:1811.08008 <a href="http://arxiv.org/abs/1811.08008" target="_blank" rel="noopener">http://arxiv.org/abs/1811.08008</a> </li>
<li>[18]  Carlos A. Gomez-Uribe and Neil Hunt. 2015. The Netfix Recommender System: Algorithms, Business Value, and Innovation. ACM Trans. Manage. Inf. Syst. 6, 4, Article 13 (Dec. 2015), 19 pages. <a href="https://doi.org/10.1145/2843948" target="_blank" rel="noopener">https://doi.org/10.1145/2843948</a> </li>
<li>[19]  Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press. <a href="http://www.deeplearningbook.org" target="_blank" rel="noopener">http://www.deeplearningbook.org</a>. </li>
<li>[20]  RuiqiGuo,SanjivKumar,KrzysztofChoromanski,andDavidSimcha.2016.Quan- tization based Fast Inner Product Search. In Proceedings of the 19th International Conference on Artifcial Intelligence and Statistics (Proceedings of Machine Learning Research), Arthur Gretton and Christian C. Robert (Eds.), Vol. 51. PMLR, Cadiz, </li>
</ol>
<p>Spain, 482–490. <a href="http://proceedings.mlr.press/v51/guo16a.html" target="_blank" rel="noopener">http://proceedings.mlr.press/v51/guo16a.html</a> </p>
<ol>
<li><p>[21]  Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural Collaborative Filtering. In Proceedings of the 26th International Conference on World Wide Web (WWW ’17). International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, Switzerland, </p>
<p>173–182. <a href="https://doi.org/10.1145/3038912.3052569" target="_blank" rel="noopener">https://doi.org/10.1145/3038912.3052569</a> </p>
</li>
<li><p>[22]  Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. </p>
<p>\2016. Session-based Recommendations with Recurrent Neural Networks. In The </p>
<p>International Conference on Learning Representations (ICLR 2016). </p>
</li>
<li><p>[23]  Y. Hu, Y. Koren, and C. Volinsky. 2008. Collaborative Filtering for Implicit Feedback Datasets. In 2008 Eighth IEEE International Conference on Data Mining. </p>
<p>263–272. <a href="https://doi.org/10.1109/ICDM.2008.22" target="_blank" rel="noopener">https://doi.org/10.1109/ICDM.2008.22</a> </p>
</li>
<li><p>[24]  AnjuliKannan,KarolKurach,SujithRavi,TobiasKaufman,BalintMiklos,Greg </p>
<p>Corrado, Andrew Tomkins, Laszlo Lukacs, Marina Ganea, Peter Young, and Vivek Ramavajjala. 2016. Smart Reply: Automated Response Suggestion for Email. In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining(KDD)(2016). <a href="https://arxiv.org/pdf/1606.04870v1.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1606.04870v1.pdf</a> </p>
</li>
<li><p>[25]  Noam Koenigstein, Parikshit Ram, and Yuval Shavitt. 2012. Efcient Retrieval of Recommendations in a Matrix Factorization Framework. In Proceedings of the 21st ACM International Conference on Information and Knowledge Management (CIKM ’12).ACM,NewYork,NY,USA,535–544. <a href="https://doi.org/10.1145/2396761.2396831" target="_blank" rel="noopener">https://doi.org/10.1145/2396761.2396831</a> </p>
</li>
<li><p>[26]  WalidKrichene,NicolasMayoraz,StefenRendle,LiZhang,XinyangYi,Lichan Hong, Ed Chi, and John Anderson. 2019. Efcient Training on Very Large Corpora via Gramian Estimation. In 7th International Conference on Learning Representations. </p>
</li>
<li><p>[27]  Jiaqi Ma, Zhe Zhao, Jilin Chen, Ang Li, Lichan Hong, and Ed H. Chi. 2019. SNR: Sub-Network Routing for Flexible Parameter Sharing in Multi-task Learning. In AAAI2019. <a href="http://www.jiaqima.com/papers/SNR.pdf" target="_blank" rel="noopener">http://www.jiaqima.com/papers/SNR.pdf</a> </p>
</li>
<li><p>[28]  JiaqiMa,ZheZhao,XinyangYi,JilinChen,LichanHong,andEdH.Chi.2018. Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture- of-Experts. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &#38; Data Mining (KDD ’18). ACM, New York, NY, USA, 1930–1939. <a href="https://doi.org/10.1145/3219819.3220007" target="_blank" rel="noopener">https://doi.org/10.1145/3219819.3220007</a> </p>
</li>
<li><p>[29]  TomasMikolov,IlyaSutskever,KaiChen,GregCorrado,andJefreyDean.2013. Distributed Representations of Words and Phrases and Their Compositionality. In Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2 (NIPS’13). Curran Associates Inc., USA, 3111–3119. http: //dl.acm.org/citation.cfm?id=2999792.2999959 </p>
</li>
<li><p>[30]  Frederic Morin and Yoshua Bengio. 2005. Hierarchical probabilistic neural net- work language model. In AISTATS’05. 246–252. </p>
</li>
<li><p>[31]  Paul Neculoiu, Maarten Versteegh, and Mihai Rotaru. 2016. Learning Text Simi- larity with Siamese Recurrent Networks. In Rep4NLP@ACL. </p>
</li>
<li><p>[32]  The Netfix Prize. 2012. The Netfix Prize. <a href="http://www.netfixprize.com/" target="_blank" rel="noopener">http://www.netfixprize.com/</a>. </p>
</li>
<li><p>[33]  S. Rendle. 2010. Factorization Machines. In 2010 IEEE International Conference on </p>
<p>Data Mining. 995–1000. <a href="https://doi.org/10.1109/ICDM.2010.127" target="_blank" rel="noopener">https://doi.org/10.1109/ICDM.2010.127</a> </p>
</li>
<li><p>[34]  Maksims Volkovs, Guangwei Yu, and Tomi Poutanen. 2017. DropoutNet: Ad- dressing Cold Start in Recommender Systems. In Advances in Neural Infor- mation Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.). Curran Associates, Inc., 4957– </p>
</li>
</ol>
<p>\4966. <a href="http://papers.nips.cc/paper/7081-" target="_blank" rel="noopener">http://papers.nips.cc/paper/7081-</a> dropoutnet- addressing- cold- start- in- </p>
<p>recommender- systems.pdf </p>
<ol>
<li><p>[35]  RuoxiWang,BinFu,GangFu,andMingliangWang.2017.Deep&amp;CrossNetwork </p>
<p>for Ad Click Predictions. In Proceedings of the ADKDD’17 (ADKDD’17). ACM, </p>
<p>NewYork,NY,USA,Article12,7pages. <a href="https://doi.org/10.1145/3124749.3124754" target="_blank" rel="noopener">https://doi.org/10.1145/3124749.3124754</a> </p>
</li>
<li><p>[36]  Xiang Wu, Ruiqi Guo, Ananda Theertha Suresh, Sanjiv Kumar, Daniel N Holtmann-Rice, David Simcha, and Felix X Yu. 2017. Multiscale Quantization for Fast Similarity Search. In Advances in Neural Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, andR.Garnett(Eds.).CurranAssociates,Inc.,5749–5757. <a href="http://papers.nips.cc/" target="_blank" rel="noopener">http://papers.nips.cc/</a> </p>
<p>paper/7157- multiscale- quantization- for- fast- similarity- search.pdf </p>
</li>
<li><p>[37]  YinfeiYang,SteveYuan,DanielCer,Sheng-YiKong,NoahConstant,PetrPilar, Heming Ge, Yun-hsuan Sung, Brian Strope, and Ray Kurzweil. 2018. Learning Semantic Textual Similarity from Conversations. In Proceedings of The Third Workshop on Representation Learning for NLP. Association for Computational Linguistics,Melbourne,Australia,164–174. <a href="https://www.aclweb.org/anthology/" target="_blank" rel="noopener">https://www.aclweb.org/anthology/</a> </p>
<p>W18- 3022 </p>
</li>
<li><p>[38]  Han Zhu, Xiang Li, Pengye Zhang, Guozheng Li, Jie He, Han Li, and Kun Gai. </p>
<p>\2018. Learning Tree-based Deep Model for Recommender Systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &#38; </p>
</li>
</ol>
<p>277 </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/19/Untitled/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="zhaochenyang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="记忆小溪">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/19/Untitled/" itemprop="url">Untitled</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-19T09:26:17+08:00">
                2019-04-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/14/基于关系及知识的embedding设计/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="zhaochenyang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="记忆小溪">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/14/基于关系及知识的embedding设计/" itemprop="url">Untitled</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-14T18:59:46+08:00">
                2019-04-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="基于关系及知识图谱的item-embedding方案设计"><a href="#基于关系及知识图谱的item-embedding方案设计" class="headerlink" title="基于关系及知识图谱的item embedding方案设计"></a>基于关系及知识图谱的item embedding方案设计</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>将视频Embedding化，即将视频转化为一个多维向量，是目前推荐领域对视频进行推荐的必要手段。通过点积或余弦距离能够快速找到类似视频。</p>
<p>本方案拟采用基于视频内容的方式，在确定空间给定任意一个视频确定向量，以达到能够冷启动的目的。另外，本方案不仅从语义层面上考虑embedding方法，而且在视频本身属性、类别等多维度进行向量调整，已达到视频不仅语义相近而且实体、类别、属性相关。</p>
<p>本方案拟采用预训练+有监督训练的方式，完成基于视频内容的embedding工作。</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><h3 id="总体目标"><a href="#总体目标" class="headerlink" title="总体目标"></a>总体目标</h3><p>本文对视频embedding主要考虑两个方面：</p>
<ol>
<li>语义相近：视频文本层面保证语义相似性</li>
<li>实体相关：同一类别、属性、实体视频embedding相关</li>
</ol>
<h3 id="Embedding方式分析"><a href="#Embedding方式分析" class="headerlink" title="Embedding方式分析"></a>Embedding方式分析</h3><p>目前Embedding有基于用户交互行为和基于内容两大方向。</p>
<ol>
<li><p>基于用户交互行为</p>
<p> 基于用户行为的推荐系统大多通过对user与item的交互行为，通过矩阵分解或类word2vec等方式将交互行为分解成user向量及item向量。像FM，item2vec，YoutubeNet，DeepWalk等，都是通过拟合用户行为完成item embedding。</p>
</li>
<li><p>基于自然语言内容</p>
<p> 基于自然语言的嵌入方法，从最简单的词袋方法，到无监督的Skip-thoughts向量，Quick-thoughts向量，到有监督的InferSent，和多任务学习微软提出的GPSR和Google提出的USE。</p>
<p> 另外，在词嵌入领域也从最早的基于分布假说的word2vec，到基于上下文词向量ELMo，再到后来更加有效的预训练模型Bert，进一步提升了自然语言处理领域多个任务效果，成为任务的预处理必备步骤。</p>
</li>
<li><p>基于图像或视频内容<br> 基于封面图像或多张视频内部图片进行图像理解，生成Tag或embedding。</p>
</li>
</ol>
<h2 id="总体设计思路"><a href="#总体设计思路" class="headerlink" title="总体设计思路"></a>总体设计思路</h2><h3 id="概要设计"><a href="#概要设计" class="headerlink" title="概要设计"></a>概要设计</h3><p>方案主要思路为通过Bert等模型预处理得到文本向量，然后进行有监督推断。简而言之，Bert+InferSent。具体来说，通过预训练Bert模型，得到视频的语义embedding，通过对语义embedding进行特征抽取，通过双塔结构完成一对样本的正负样本预测或采用Triplet loss优化模型。</p>
<h3 id="模型设计总图"><a href="#模型设计总图" class="headerlink" title="模型设计总图"></a>模型设计总图</h3><p>以下<br><img src="https://ws3.sinaimg.cn/large/005BYqpggy1g2ckr6kaacj30ro0qwwg4.jpg =400x300" alt=""><br><img src="https://ws3.sinaimg.cn/large/005BYqpgly1g2ckppw53bj30h809xwfn.jpg =500x250" alt=""></p>
<h2 id="详细设计"><a href="#详细设计" class="headerlink" title="详细设计"></a>详细设计</h2><h3 id="数据源"><a href="#数据源" class="headerlink" title="数据源"></a>数据源</h3><p>基本数据信息：</p>
<p>hive表：    ods_up_content_source, ods_up_content_cms_day</p>
<p>nlp topic: simba_features_raw</p>
<h3 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h3><p>数据准备：</p>
<ol>
<li>相关字段：title， content， label， keywords， category， author name， author id，classes， sub classes</li>
<li>文本字段：title， content</li>
<li>label字段：category，author id，sub classes，（keywords，人名，节目名等人造名）</li>
<li>正负样本准备：同一类目内的视频互为正样本，不同类目互为负样本，控制不同类目的采样数量，比例。</li>
<li>文本相似样本可沿用原新浪新闻title-&gt;summary。</li>
</ol>
<h3 id="模型及目标设计"><a href="#模型及目标设计" class="headerlink" title="模型及目标设计"></a>模型及目标设计</h3><p>模型设计</p>
<ol>
<li>使用预训练bert词向量的作为基本词向量，对词向量做average，max-pooling等方式得到句向量（Bag of Words）</li>
<li>使用预训练bert词向量的作为基本词向量，对词向量做average，max-pooling等方式得到句向量，然后使用dnn 1-2 层全连接层，得到句向量（DNN）</li>
<li>使用预训练bert词向量的作为基本词向量，对词向量做average，max-pooling等方式得到句向量，然后使用inferSent 模型中bi-LSTM max-pooling方式得到句向量（InferSent）</li>
</ol>
<p><img src="https://ws3.sinaimg.cn/large/005BYqpgly1g2ckidt4raj311i0qs0wf.jpg =500x400" alt=""><br><img src="https://ws3.sinaimg.cn/large/005BYqpgly1g2cktqz0dej30vq0u043w.jpg =500x400" alt=""><br><img src="https://ws3.sinaimg.cn/large/005BYqpgly1g2ckujdrz9j30zu0k4jst.jpg =300x200" alt="triplet loss"></p>
<p>得到句向量后可以通过如下方式优化模型</p>
<ol>
<li>通过inferSent的方式，对句向量进行点积，差值绝对值等，通过DNN层，进入softmax层进行分类。（softmax）</li>
<li>通过三元组（Item, Item+, Item-），Loss(Item, Item+)&lt;Loss(Item, Item-)完成模型优化。（Triplet loss）</li>
</ol>
<h2 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h2><p>通过Bert预训练模型得到embedding后，输入encoder部分，得到embedding。</p>
<h2 id="评价方法"><a href="#评价方法" class="headerlink" title="评价方法"></a>评价方法</h2><h3 id="离线评价"><a href="#离线评价" class="headerlink" title="离线评价"></a>离线评价</h3><p>离线通过分类准确率的方式进行评价。或通过rank@k的方式进行评价。</p>
<h3 id="在线使用与评价"><a href="#在线使用与评价" class="headerlink" title="在线使用与评价"></a>在线使用与评价</h3><p>Item Embedding 使用方式主要有以下几种：</p>
<ol>
<li>通过Item 查找Item 的方式，即在相关推荐，类似视频等产品位，直接以item 查找最近item的方式使用，以线上有效视频点击，平均观看时长等指标衡量。</li>
<li>作为DNN的输入，通过改善模型效果的方式。具体指标以模型指标为准。</li>
</ol>
<h2 id="具体工作"><a href="#具体工作" class="headerlink" title="具体工作"></a>具体工作</h2><ol>
<li>数据准备：从hive表中抽取数据，完成数据清洗</li>
<li>Bert词向量抽取：给定文本信息，利用bert预训练模型，产生最后4层embedding词向量</li>
<li>模型搭建：利用tensorflow由简单到复杂，搭建embedding模型</li>
<li>模型实验：根据评价指标，对不同模型进行评价，选择最优模型。</li>
<li>模型预测：搭建整体模型实时预测流程，文本-&gt;bert embedding-&gt;final embedding。</li>
<li>数据入赤霄索引库：根据输入特征信息，完成embedding预测，在按照赤霄入库流程完成embedding入库。</li>
</ol>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol>
<li><a href="https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a" target="_blank" rel="noopener">The Current Best of Universal Word Embeddings and Sentence Embeddings</a></li>
<li><a href="https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf" target="_blank" rel="noopener">Matrix Factorization Techniques for Recommender Systems</a></li>
<li><a href="https://github.com/google-research/bert" target="_blank" rel="noopener">Bert Code</a></li>
<li><a href="https://arxiv.org/abs/1403.6652" target="_blank" rel="noopener">DeepWalk: Online Learning of Social Representations</a></li>
<li><a href="https://arxiv.org/abs/1603.04259" target="_blank" rel="noopener">Item2Vec: Neural Item Embedding for Collaborative Filtering</a></li>
<li><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a></li>
<li><a href=""></a></li>
<li><a href=""></a></li>
<li><a href=""></a></li>
<li><a href=""></a></li>
<li><a href=""></a></li>
<li><a href=""></a>7. </li>
<li><a href="https://arxiv.org/abs/1803.11175" target="_blank" rel="noopener">Google’s Universal Sentence Encoder</a></li>
<li><a href="https://arxiv.org/abs/1804.00079" target="_blank" rel="noopener">General Purpose Sentence Representation</a></li>
<li><a href="https://arxiv.org/abs/1705.02364" target="_blank" rel="noopener">InferSent</a></li>
<li><a href="https://openreview.net/forum?id=rJvJXZb0W" target="_blank" rel="noopener">Quick-thoughts vectors</a></li>
<li><a href="https://arxiv.org/abs/1506.06726" target="_blank" rel="noopener">Skip-thoughts vectors</a></li>
<li><a href="https://allennlp.org/elmo" target="_blank" rel="noopener">ELMo</a></li>
<li><a href="https://mc.ai/how-deep-does-your-sentence-embedding-model-need-to-be/" target="_blank" rel="noopener">How deep does your Sentence Embedding model need to be ?</a></li>
<li><a href="http://jalammar.github.io/illustrated-bert/" target="_blank" rel="noopener">The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/08/embedding-based_news_recommendation_for_millions_of_users/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="zhaochenyang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="记忆小溪">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/08/embedding-based_news_recommendation_for_millions_of_users/" itemprop="url">Untitled</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-08T17:23:55+08:00">
                2019-04-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Embedding-based-News-Recommendation-for-Millions-of-Users"><a href="#Embedding-based-News-Recommendation-for-Millions-of-Users" class="headerlink" title="Embedding-based News Recommendation for Millions of Users"></a>Embedding-based News Recommendation for Millions of Users</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>本文提出三步端到端的方法</p>
<ol>
<li>基于denoising autoencoder 开始文章的分布式表示</li>
<li>基于浏览历史作为输入序列用RNN产生用户表示</li>
<li>匹配并列出基于点击操作的文章列表</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/10/elastic_dump/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="zhaochenyang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="记忆小溪">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/10/elastic_dump/" itemprop="url">elasticdump</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-12-10T13:43:29+08:00">
                2018-12-10
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>开源工具地址： <a href="https://github.com/taskrabbit/elasticsearch-dump" target="_blank" rel="noopener">https://github.com/taskrabbit/elasticsearch-dump</a></p>
<p>查看所有index<br>线上 curl ‘10.0.12.100:6335/_cat/indices?v’</p>
<p>同步mytestindex从线上环境到测试环境</p>
<p>导出map<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">elasticdump \</span><br><span class="line">    --input=http://10.0.12.100:9200/mytestindex \</span><br><span class="line">    --output=./mytestindex.map \</span><br><span class="line">    --type=mapping</span><br></pre></td></tr></table></figure></p>
<p>导出数据<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">elasticdump \</span><br><span class="line">    --input=http://10.0.12.100:9200/mytestindex \</span><br><span class="line">    --output=./mytestindex.json</span><br></pre></td></tr></table></figure></p>
<pre><code>导入map
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">elasticdump –input=./mytestindex.map –output=http://10.1.1.246:9200/mytestindex –type=mapping</span><br></pre></td></tr></table></figure>
<p>导入数据<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">elasticdump \</span><br><span class="line">    –input=./mytestindex.json \</span><br><span class="line">    –output=http://10.1.1.246:9200/mytestindex –limit 1000</span><br></pre></td></tr></table></figure></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="http://whatua.com/2018/02/28/elasticsearch-dump-%E7%BA%BF%E4%B8%8A%E4%B8%8B%E7%BA%BF%E7%8E%AF%E5%A2%83%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E5%B7%A5%E5%85%B7/" target="_blank" rel="noopener">es-同步工具</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/11/15/sentence_embedding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="zhaochenyang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="记忆小溪">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/15/sentence_embedding/" itemprop="url">Untitled</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-15T13:37:04+08:00">
                2018-11-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Word-Eembedding"><a href="#Word-Eembedding" class="headerlink" title="Word Eembedding"></a>Word Eembedding</h2><p>在NLP领域近年来有个默认的标准是，使用word embedding作为词语的初始化权重，从2013年以来Word2vec, GloVe, FastText, ELMo等embedding方法相继问世，从静态embedding到动态embedding，为后续任务提供了强力,</p>
<h2 id="Sentence-Embedding"><a href="#Sentence-Embedding" class="headerlink" title="Sentence Embedding"></a>Sentence Embedding</h2><p>句子向量表示是很多问题的基础问题，如图像领域的图片表示问题一样，但前几年进展不大，一直有一种做法是直接把word embedding平均作为句向量的表示，而前几年的方法都没有比这个基线超出太多。但实际上，在生产环境，word embedding平均作为句向量有很多弊端，首先是每个词的权重并不一样，且没有语序等关系加入，使得效果一直不理想。当然，最大的问题还是word embedding不能区分歧义词所带来问题，大部分badcase是多义词带来的。包括后续下游任务中使用word embedding等也没有处理多义词问题。</p>
<p>近一段时间来，在句子向量表示领域有很大进展，包含无监督算法、有监督算法、多任务算法等。</p>
<h3 id="无监督方法"><a href="#无监督方法" class="headerlink" title="无监督方法"></a>无监督方法</h3><p>先来介绍最普通的方式Bag of words，就是根据word embedding，简单相加得到 sentence embedding。缺点之一是没有权重。</p>
<p>后来有变种，通过TF-IDF值作为词的权重，加权平均，得到 sentence embedding。</p>
<h3 id="有监督的方法"><a href="#有监督的方法" class="headerlink" title="有监督的方法"></a>有监督的方法</h3><p>各种DNN、RNN、CNN得到的模型。</p>
<h2 id="Evaluation-Metrics"><a href="#Evaluation-Metrics" class="headerlink" title="Evaluation Metrics"></a>Evaluation Metrics</h2><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><p><a href="https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a" target="_blank" rel="noopener">The Current Best of Universal Word Embeddings and Sentence Embeddings</a></p>
</li>
<li><p><a href="http://nlp.town/blog/sentence-similarity/" target="_blank" rel="noopener">sentence-similarity</a></p>
</li>
</ol>
<ul>
<li><p>SIF references:<br>Sanjeev Arora, Yingyu Liang and Tengyu Ma, A Simple but Tough-to-Beat Baseline for Sentence Embeddings</p>
</li>
<li><p>Word2vec references:<br>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient Estimation of Word Representations in Vector Space. In Proceedings of Workshop at ICLR, 2013.</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS, 2013.</p>
<p>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic Regularities in Continuous Space Word Representations. In Proceedings of NAACL HLT, 2013.</p>
</li>
<li><p>GloVe references:<br>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation.</p>
</li>
<li><p>FastText references:<br>P. Bojanowski, E. Grave, A. Joulin, T. Mikolov, Enriching Word Vectors with Subword Information</p>
</li>
<li><p>ELMo references:<br>Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer. Deep contextualized word representations NAACL 2018.</p>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/18/ANN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="zhaochenyang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="记忆小溪">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/18/ANN/" itemprop="url">Untitled</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-18T17:43:31+08:00">
                2018-10-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<h2 id="title-大规模高维数据检索算法"><a href="#title-大规模高维数据检索算法" class="headerlink" title="title:大规模高维数据检索算法"></a>title:大规模高维数据检索算法</h2><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>给定一个</p>
<h2 id="LSH"><a href="#LSH" class="headerlink" title="LSH"></a>LSH</h2><p>LSH (Local Sensitive Hash) 是一种在大规模数据中寻找相似数据的方法. LSH 算是一种随机算法 (Random Algorithm). Random Algorithm 的特点是, 它不能保证每次都返回准确的结果, 但是, 它是已较高的概率保证返回正确的结果, 或者是与正确结果很接近的 value. 而且, 如果计算资源足够, 概率可以 as high as desired. LSH 以小概率的搜索失败为代价, 大幅度降低了搜索时间复杂度.</p>
<h2 id="k-d-tree"><a href="#k-d-tree" class="headerlink" title="k-d tree"></a>k-d tree</h2><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a href="http://shuokay.com/2016/07/26/lsh/" target="_blank" rel="noopener">大规模高维数据检索算法 LSH</a></li>
<li><a href="http://www.cnblogs.com/eyeszjwang/articles/2429382.html" target="_blank" rel="noopener">k-d tree算法</a></li>
<li><a href="https://ask.julyedu.com/question/7312" target="_blank" rel="noopener">ANN 总结</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/15/surprise/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="zhaochenyang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="记忆小溪">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/15/surprise/" itemprop="url">Untitled</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-15T09:39:40+08:00">
                2018-10-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<h2 id="title-Surprise"><a href="#title-Surprise" class="headerlink" title="title:Surprise"></a>title:Surprise</h2>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/11/deep_recommendations/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="zhaochenyang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="记忆小溪">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/11/deep_recommendations/" itemprop="url">深度推荐系统</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-11T18:01:39+08:00">
                2018-10-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="推荐系统与推荐模型"><a href="#推荐系统与推荐模型" class="headerlink" title="推荐系统与推荐模型"></a>推荐系统与推荐模型</h2><h3 id="常见推荐模型"><a href="#常见推荐模型" class="headerlink" title="常见推荐模型"></a>常见推荐模型</h3><ul>
<li>协同过滤<br>** MF算法</li>
<li>内容推荐</li>
<li>混合模型(协同+内容)</li>
</ul>
<h3 id="推荐系统的系统架构"><a href="#推荐系统的系统架构" class="headerlink" title="推荐系统的系统架构"></a>推荐系统的系统架构</h3><ul>
<li>召回+排序</li>
</ul>
<h2 id="基于表示学习的深度推荐模型"><a href="#基于表示学习的深度推荐模型" class="headerlink" title="基于表示学习的深度推荐模型"></a>基于表示学习的深度推荐模型</h2><ul>
<li></li>
</ul>
<h2 id="基于特征组合的深度推荐模型"><a href="#基于特征组合的深度推荐模型" class="headerlink" title="基于特征组合的深度推荐模型"></a>基于特征组合的深度推荐模型</h2>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/25/gated_rnn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="zhaochenyang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="记忆小溪">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/25/gated_rnn/" itemprop="url">Untitled</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-09-25T09:21:23+08:00">
                2018-09-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<p>title:Gated RNN and Sequence Generation</p>
<hr>
<h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><hr>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">zhaochenyang</p>
              <p class="site-description motion-element" itemprop="description">remember & think</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">25</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">16</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">zhaochenyang</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
